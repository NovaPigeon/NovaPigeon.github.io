<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>CrystalNet</title>
    <url>/2024/05/15/CrystalNet/</url>
    <content><![CDATA[<h1 id="faithfully-emulating-large-production-networks">Faithfully
Emulating Large Production Networks</h1>
<p>Please read the
https://www.microsoft.com/en-us/research/wp-content/uploads/2017/10/p599-liu.pdf
(you can focus on S1-S5), and try your best to finish the following
three tasks:</p>
<h2 id="task-1-reading-writing">Task #1: Reading &amp; Writing: </h2>
<h3 id="what-problem-is-this-paper-solving">1. What problem is this
paper solving?</h3>
<p>对于云服务和 online service
的大型网络而言，可靠地运行是一项挑战。这些网络往往包含数以万计的异构的设备，且一直在动态变化中。这使得哪怕是一些小问题，诸如硬件故障，软件bug，配置错误和人为的小错误（如输错命令），也会导致严重的后果，譬如网络中断。SLA
的不可靠继而导致用户的流失，这对服务提供商来说是灾难性的。</p>
<p>一个可行的思路是，在高保真的网络仿真器中验证所有网络操作，然后再在生产网络中执行。以往的技术并没有办法实现这种思路。</p>
<ol type="1">
<li>Small hardware testbeds，如 Cloudlab/
Emulab，只能对新设备进行单元/压力测试，但无法模拟复杂网络拓扑结构中的相互关系。</li>
<li>Network verification tools，例如
batfish，是根据设备配置和拓扑，来模拟理想状态下的路由的。但它无法揭示设备固件
bug
和诸如不同供应商对同一路由协议的实现差异带来的问题，且无法防止人为错误（因为其工作流程与生产网络截然不同，无法方便地起到预演的作用）。</li>
<li>Small-scale network emulator，如
MiniNet/GNS，拥有许多缺陷（如不支持异构的设备固件，不识别安全的仿真边界），且无法扩展到如大型云网络的规模。</li>
</ol>
<p>因此，本文提出了 highly-scalable/high-fidelity network emulator:
CrystalNet.</p>
<h3 id="what-is-the-key-idea">2. What is the key idea?</h3>
<p>CrystalNet 首先需要建立物理网络的模拟。</p>
<p>对于异构的网络设备，CrystalNet 将设备运行在 Docker
容器中，部署在云上的 VM 里。另外，实现一个 PhyNet 容器层，包含 virtual
interface，设备容器与 PhyNet 连接，而网络拓扑通过 PhyNet
之间的互相连接实现。因此不用针对每个设备软件 image 的黑箱一一实现
API，而只要实现统一的 API；设备软件像与物理 interface 交互一样与 PhyNet
中的 virtual interface 交互。</p>
<p>对于只提供 VM image 而不提供容器 image 的设备，CrystalNet 将 VM image
等打包到一个新的容器 image 中，再在云 VM 中运行该
image（云需要支持嵌套虚拟机）</p>
<p>CrystalNet 也支持真实硬件的耦合，硬件交换机与 PhyNet 相连（经过
fanout 交换机），就可以与其他仿真设备交互。</p>
<p>对于 CrystalNet
的数据平面虚拟链路，仿真设备将其视为以太网链路。且使用 VXLAN
协议，模拟以太网链路(L2-L3，添加 UDP 报头，可连接 IP
网络)，使数据在真实网络中传播，而这对仿真设备透明。</p>
<p>对于 CrystalNet 的控制平面虚拟链路，CrystalNet 部署了一个 Linux
Jumpbox，与所有仿真设备相连。通过
Jumpbox，操作者可以像在生产环境中一样操作所有仿真设备。</p>
<p>任何仿真网络都需要有边界，一则没有那么多的资源，二则无法获知控制范围之外的网络设备的信息。CrystalNet
定义了安全静态边界的概念(safe static
boundaries)，指当内部的仿真设备发生各种变动时，外部网络可以视作始终保持静态。模拟外部网络时，CrystalNet只模拟与内部设备直接相连的设备，称为
speaker device，这些设备是静态的，只通过基本的活动（如
ARP）与边界设备保持连接，且可以发送任何路由信息。CrystalNet 针对
BGP/OSFP/SDN，提出了安全静态边界成立的一些充分条件。并对于 Clos-like
&amp; BGP 的数据中心网络(其拓扑可以视作 multi-root
tree)提出了一种搜索安全静态边界的算法。经过实验，计算安全静态边界降低了
90% 以上的成本。</p>
<p>通过以上设计，CrystalNet 实现了以下目标： 1. 可以运行在公有云上，易于
scale out（container） 2. 可以透明地模拟物理网络（PhyNet, VXLAN） 3.
可以透明地模拟外部网络（安全静态边界，speaker device）</p>
<h3 id="any-idea-to-improve-the-proposed-solution">3. Any idea to
improve the proposed solution?</h3>
<ol type="1">
<li>当前 CrystalNet 的用法还是一种网络的验证工具，那是否有可能将
CrystalNet 作为生产环境与实际操作之间的缓冲层？CrystalNet
与生产网络同构，每当执行一个操作，首先自动化地在 CrystalNet
上执行，然后经过一段时间的验证后，再从 CrystalNet 导向实际生产网络。另外
CrystalNet
应当对操作者透明，除了报错时，操作者应当意识不到他们操作的究竟是
CrystalNet 还是真实网络。</li>
<li>模拟数据平面的内容，诸如丢包率，延迟，带宽。</li>
<li>支持除 Ethernet
网卡外的其他设备和除以太网之外的其他链路层协议。</li>
<li>改进仿真设备在云上的部署算法，考虑延迟等度量的同构性。 (2-4
详见问4中所述)</li>
</ol>
<h3 id="ask-three-questions-you-dont-understand-about-the-paper.">4. Ask
three questions you don’t understand about the paper.</h3>
<ol type="1">
<li>此处说到的可以扩展到其他设备，包括其他类型的硬件接口吗？譬如
InfiniBand 版的 RDMA 实现就重构了链路层，但前文中提到 CrystalNet 只支持
Ethernet 网卡。其他的譬如各式 SmartNIC，DPU
之类的可以与之兼容吗？（这似乎不只是软件层面的问题，还有硬件的异构）CrystalNet
可以支持不同的链路层协议吗？</li>
<li>数据平面的丢包率，延迟，带宽等为什么难以模拟呢？如 Linux 提供的
tc-netem，就可以模拟丢包率，延迟，带宽等等。</li>
<li>在公有云上部署 VMs，这些 VMs
的实际物理位置是不可知的（甚至会跨越多个公有云），这会导致实际上被部署在一起、相互之间延迟极低的设备，在云上可能相距极远、延迟极高。（云上容器的实际拓扑与生产环境中设备的拓扑不相符）进一步导致诸如
OSPF 等对 time cost
敏感的路由协议在更新路由表、计算路由路径时与实际可能发生的情况不相符。如何处理这种情况，直接写死路由吗？或许可以有部署算法的改进可能？</li>
</ol>
<h2 id="task-2-math-formulation">Task #2: Math formulation: </h2>
<h3
id="could-you-mathematically-formulate-the-problem-discussed-in-sec-5">1.
Could you mathematically formulate the problem discussed in Sec 5?</h3>
<p>Define the global network topology as an undirected graph <span
class="math inline">\(G=(V,L)\)</span>, where <span
class="math inline">\(V\)</span> is the set of device nodes, and <span
class="math inline">\(L\)</span> represents the links between devices
(also the undirected edges in the graph).</p>
<p>Define emulated devices <span class="math inline">\(D\)</span>,
external devices <span class="math inline">\(E\)</span>, where <span
class="math inline">\(G=D \cup E \wedge D\cap E=\emptyset\)</span>.</p>
<p>Define boundary devices <span class="math inline">\(B :=\{u|(u\in D)
\wedge (\exists v\in E,s.t. \space (u,v)\in L)\}\)</span>, which are
emulated devices directly connected to external devices.</p>
<p>Define internal devices <span class="math inline">\(I:=\{u|(u\in
D)\wedge(\forall v\in E, (u,v)\notin L)\}\)</span>, which are emulated
devices whose neighbors are all emulated devices. Thus, <span
class="math inline">\(B\cup I=D \wedge B\cap I=\emptyset\)</span>.</p>
<p>Define speaker devices (speaker device) <span
class="math inline">\(S:=\{u|(u\in E)\wedge(\exists v\in D,s.t. (u,v)\in
L)\}\)</span>, which are external devices directly connected to emulated
devices.</p>
<p>Define the boundary <span class="math inline">\(B\)</span> as a safe
static boundary: all behaviors of emulated devices are consistent with
the real network. Since CrystalNet implements speaker devices as static,
this statement is equivalent to the fact that the behavior of speaker
devices in the real network cannot affect devices inside the boundary
(including boundary devices).</p>
<p>Define an impact function <span class="math inline">\(f: V\times
P(V\times V) \to P(V)\)</span>, where <span
class="math inline">\(P(V)\)</span> is the power set of <span
class="math inline">\(V\)</span>. <span class="math inline">\(\forall
u\in V\)</span>, <span class="math inline">\(f(u, R)=Next_{u,R}\subseteq
V\)</span>, and for all <span class="math inline">\(v\in
Next_{u,R}\)</span>, <span class="math inline">\((u,v)\in L\)</span>.
This means that if <span class="math inline">\(u\)</span> wants to send
information, this information can propagate to devices in <span
class="math inline">\(Next_{u,R}\)</span>, and these devices will
continue to apply the function <span class="math inline">\(f(v,R\cup \{
&lt;u,v&gt;\})\)</span> to send information. Here, <span
class="math inline">\(R\)</span> records the path that the information
propagation has traversed.</p>
<p>We apply the following algorithm to determine whether <span
class="math inline">\(B\)</span> is a safe boundary.</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">global</span> V,L,D,E,f <span class="comment"># given</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">transmitt</span>(<span class="params">u,R</span>)-&gt;<span class="built_in">bool</span>:</span><br><span class="line">    Next_u_R=f(u,R)</span><br><span class="line">    <span class="keyword">for</span> v <span class="keyword">in</span> Next_u_R:</span><br><span class="line">        <span class="keyword">if</span> u <span class="keyword">in</span> E <span class="keyword">and</span> v <span class="keyword">in</span> D:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">    <span class="keyword">if</span> transmitt(f,v,R.append(&lt;u,v&gt;))==<span class="literal">False</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">    </span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">isSafeStaticBoundary</span>()-&gt;<span class="built_in">bool</span>:</span><br><span class="line">    <span class="keyword">for</span> begin <span class="keyword">in</span> D:</span><br><span class="line">        <span class="keyword">if</span> transmitt(begin,[])==<span class="literal">False</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">True</span></span><br></pre></td></tr></table></figure>
<p>This problem may not be Turing-decidable because it may produce
loops, and <span class="math inline">\(f\)</span> is a function that may
behave differently based on past path information, making it impossible
to determine when duplicates will occur. Therefore, the algorithm may
not stop. Of course, if a deterministic <span
class="math inline">\(f\)</span> is provided, the result can be
determined through constraints.</p>
<h3
id="given-your-problem-formulation-and-notations-could-you-formally-express-the-lemma-and-propositions-in-sec-5">2.
Given your problem formulation and notations, could you formally express
the Lemma and Propositions in Sec 5?</h3>
<h4 id="bgp">BGP</h4>
<p>For networks using BGP protocol, we need to add AS information. Let
<span class="math inline">\(ASes:= \{AS_{n_i}\}\)</span>, <span
class="math inline">\(ASN=\{n_i\}\)</span>, where <span
class="math inline">\(\cup_{i}AS_{n_i}=V\)</span>, and <span
class="math inline">\(\cap_{i}AS_{n_i}=\emptyset\)</span>.</p>
<p>We can simplify <span class="math inline">\(R\)</span> as an ordered
set of nodes in the path.</p>
<p>Define <span class="math display">\[inSameAS: V\times V\rightarrow
\{true,false\}\]</span></p>
<p><span class="math display">\[inSameAS(u,v)= \exists i,s.t.\space u\in
AS_{n_i} \wedge v\in AS_{n_i}\]</span></p>
<p>The propagation function is given by: <span
class="math display">\[f(u,R)=Next_{u,R}=\{v|((u,v)\in L)\wedge
(\nexists w \in R,s.t. \space inSameAS(w,v))\wedge
(!inSameAS(u,v))\}\]</span></p>
<p>Subsequently, <span class="math display">\[\forall v \in Next_{u,R}
\text{, call } {f(v,R\cup \{ u\})}\]</span></p>
<p><span class="math inline">\(RV\)</span> records the paths that appear
in the propagation.</p>
<p>The problem then transforms into the following algorithm:</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">global</span> V,L,D,E,ASes <span class="comment"># given</span></span><br><span class="line"></span><br><span class="line">RV=[] <span class="comment"># route vectors</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">transmitt</span>(<span class="params">u,R</span>)-&gt;<span class="built_in">bool</span>:</span><br><span class="line">    Next_u_R=[]</span><br><span class="line">    RV.append(R.append(u))</span><br><span class="line">    <span class="keyword">for</span> v <span class="keyword">in</span> Neighbor(u):</span><br><span class="line">        flag=<span class="literal">False</span></span><br><span class="line">        <span class="keyword">if</span> inSameAS(u,v):</span><br><span class="line">            flag=true</span><br><span class="line">    <span class="keyword">for</span> w <span class="keyword">in</span> R:</span><br><span class="line">        <span class="keyword">if</span> inSameAS(v,w):</span><br><span class="line">            flag=<span class="literal">True</span></span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        <span class="keyword">if</span> flag==<span class="literal">True</span></span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        <span class="keyword">if</span> u <span class="keyword">in</span> E <span class="keyword">and</span> v <span class="keyword">in</span> D:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">        Next_u_R.append(v)</span><br><span class="line">    <span class="keyword">for</span> v <span class="keyword">in</span> Next_u_R:</span><br><span class="line">        <span class="keyword">if</span> transmitt(v,R.append(u))==<span class="literal">False</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">    </span><br><span class="line"></span><br><span class="line"> <span class="keyword">def</span> <span class="title function_">isSafeStaticBoundary</span>()-&gt;<span class="built_in">bool</span>:</span><br><span class="line">    <span class="keyword">for</span> begin <span class="keyword">in</span> D:</span><br><span class="line">        <span class="keyword">if</span> transmitt(begin,[],V,L,D,E)==<span class="literal">False</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><span class="math inline">\(\text{Lemma 5.1: A boundary is safe if
and only if }\forall R \in RV, \nexists i\text{, s.t.}R[i] \in E \wedge
R[i+1] \in D.\)</span></p>
<p><span class="math inline">\(\text{Proposition 5.2: If }\exists
i,\text{ s.t. }B \subseteq AS_{n_i} \wedge (\nexists u,v \in S , u\not
={v}\text{, s.t. }inSameAS(u,v))\text{, the boundary is
safe.}\)</span></p>
<p><span class="math inline">\(\text{Proposition 5.3: If }\forall R \in
RV, \nexists i&lt;j&lt;k\text{, s.t. }(R[i] \text{ and } R[k] \in B)
\wedge (R[j] \in E)\text{ the boundary is safe.}\)</span></p>
<h4 id="ospf">OSPF</h4>
<p>Let <span class="math inline">\(S,B,L,D,E\)</span> all be time
series.</p>
<p>Let <span class="math inline">\(Links(S_{t_i},B_{t_i}):=\{(u,v)|(u,v)
\in L_{t_i} \wedge u \in S_{t_i} \wedge v \in B_{t_i}\}\)</span></p>
<p><span class="math inline">\(\text{Proposition 5.4: If }\forall i,
Links(S_{t_i},B_{t_i})=Links(S_{t_0},B_{t_0}) \text{ and }DR,BDR \in
D\text{, the boundary of the OSPF network is safe.}\)</span></p>
<h2
id="task-3-programming-could-you-implement-and-test-algorithm-1-with-a-sample-network">Task
#3: Programming: Could you implement and test Algorithm 1 with a sample
network?</h2>
<p>Please refer to the file algorithm.ipynb. The search algorithm is
implemented based on a multi-root tree.</p>
<h3 id="generate-multi-root-tree">Generate multi-root tree</h3>
<p>将 Clos-like 的数据中心网络拓扑结构看作 multi-root tree，随机生成 DAG
G。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> networkx <span class="keyword">as</span> nx</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">generate_multi_root_tree</span>(<span class="params">num_roots:<span class="built_in">int</span>, depth:<span class="built_in">int</span></span>)-&gt;nx.classes.digraph.DiGraph:</span><br><span class="line">    G = nx.DiGraph()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 添加根节点</span></span><br><span class="line">    roots = ([<span class="string">f&quot;root_<span class="subst">&#123;i&#125;</span>&quot;</span> <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_roots)])</span><br><span class="line">    options=&#123;<span class="string">&quot;depth&quot;</span>:<span class="number">0</span>&#125;</span><br><span class="line">    G.add_nodes_from(roots,**options)</span><br><span class="line">    upper_layer=roots</span><br><span class="line">    <span class="keyword">for</span> d <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>,depth+<span class="number">1</span>):</span><br><span class="line">        num_nodes=random.randint(<span class="number">3</span>,<span class="number">8</span>)</span><br><span class="line">        nodes = ([<span class="string">f&quot;node_<span class="subst">&#123;d&#125;</span>_<span class="subst">&#123;i&#125;</span>&quot;</span> <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_nodes)])</span><br><span class="line">        options=&#123;<span class="string">&quot;depth&quot;</span>:d&#125;</span><br><span class="line">        G.add_nodes_from(nodes,**options)</span><br><span class="line">        <span class="keyword">for</span> node <span class="keyword">in</span> nodes:</span><br><span class="line">            n_links=random.randint(<span class="number">0</span>,<span class="built_in">len</span>(upper_layer))</span><br><span class="line">            rand_parents=random.sample(upper_layer,n_links)</span><br><span class="line">            edge_list=[(node,parent) <span class="keyword">for</span> parent <span class="keyword">in</span> rand_parents]</span><br><span class="line">            G.add_edges_from(edge_list)</span><br><span class="line">        upper_layer=nodes</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> G</span><br><span class="line"></span><br><span class="line">G = generate_multi_root_tree(<span class="number">3</span>, <span class="number">5</span>)</span><br><span class="line">pos = nx.layout.multipartite_layout(G,subset_key=<span class="string">&#x27;depth&#x27;</span>,align=<span class="string">&quot;horizontal&quot;</span>)</span><br><span class="line">nx.draw(G, with_labels=<span class="literal">True</span>,pos=pos)</span><br></pre></td></tr></table></figure>
<h3 id="generate-must-have-device">Generate must-have device</h3>
<p>从 G 中按比率 p 随机 sample
一些点，作为必须要仿真的设备。在图中用绿色标识。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">genarate_must_have_device</span>(<span class="params">G:nx.classes.digraph.DiGraph,p=<span class="number">0.2</span></span>)-&gt;<span class="built_in">list</span>:</span><br><span class="line">    D=[]</span><br><span class="line">    <span class="keyword">for</span> node <span class="keyword">in</span> G.nodes():</span><br><span class="line">        r=random.randint(<span class="number">0</span>,<span class="number">100</span>)</span><br><span class="line">        <span class="keyword">if</span> r&lt;<span class="number">100</span>*p:</span><br><span class="line">            D.append(node)</span><br><span class="line">    <span class="keyword">return</span> D</span><br><span class="line"></span><br><span class="line">D=genarate_must_have_device(G)</span><br><span class="line">pos = nx.layout.multipartite_layout(G,subset_key=<span class="string">&#x27;depth&#x27;</span>,align=<span class="string">&quot;horizontal&quot;</span>)</span><br><span class="line">nx.draw(G, with_labels=<span class="literal">True</span>,pos=pos)</span><br><span class="line">nx.draw_networkx_nodes(G, pos, nodelist=D, node_color=<span class="string">&#x27;green&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h3 id="search-static-safe-boundary-of-datacenter">Search static safe
boundary of datacenter</h3>
<p>搜索保证边界安全静态要仿真的所有设备，在图中用黄色标识。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">find_safe_dc_boundary</span>(<span class="params">G:nx.classes.digraph.DiGraph,D:<span class="built_in">list</span></span>)-&gt;<span class="built_in">list</span>:</span><br><span class="line">    D_=<span class="built_in">set</span>()</span><br><span class="line">    D=<span class="built_in">set</span>(D)</span><br><span class="line">    <span class="keyword">while</span> <span class="built_in">len</span>(D)!=<span class="number">0</span>:</span><br><span class="line">        d=D.pop()</span><br><span class="line">        D_.add(d)</span><br><span class="line">        depth=G.nodes[d][<span class="string">&#x27;depth&#x27;</span>]</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> depth==<span class="number">0</span>:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        upper_devs=<span class="built_in">list</span>(G.neighbors(d))</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> ud <span class="keyword">in</span> upper_devs:</span><br><span class="line">            <span class="keyword">if</span> ud <span class="keyword">not</span> <span class="keyword">in</span> D.union(D_):</span><br><span class="line">                D.add(ud)</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">list</span>(D_)</span><br><span class="line"></span><br><span class="line">D_=find_safe_dc_boundary(G,D) </span><br><span class="line">pos = nx.layout.multipartite_layout(G,subset_key=<span class="string">&#x27;depth&#x27;</span>,align=<span class="string">&quot;horizontal&quot;</span>)</span><br><span class="line">nx.draw(G, with_labels=<span class="literal">True</span>,pos=pos)</span><br><span class="line">nx.draw_networkx_nodes(G, pos, nodelist=D_, node_color=<span class="string">&#x27;yellow&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Paper Reading</category>
      </categories>
      <tags>
        <tag>Network</tag>
        <tag>Network Emulating</tag>
      </tags>
  </entry>
  <entry>
    <title>Distilling Step-by-Step!</title>
    <url>/2024/05/19/Distilling-Step-by-Step/</url>
    <content><![CDATA[<h2 id="background-knowledge">Background Knowledge</h2>
<h3 id="knowledge-distillation">Knowledge Distillation</h3>
<p>Knowledge Distillation, 知识蒸馏，是一种模型压缩技术。
如图所示，一个小的 student model 模仿一个大的 teacher model，利用
teacher model 所蕴含的知识获得相似或更高的准确度。
我们可以利用这种技术，将大模型压缩为可以灵活部署的小模型。</p>
<img src="/2024/05/19/Distilling-Step-by-Step/p0.png" class="">
<h3 id="cotchain-of-thought">CoT(Chain of Thought)</h3>
<p>CoT，思维链。
就是说让大模型逐步将一个复杂问题分解为子问题并依次求解，可以显著提升模型的性能。
传统的 prompt 产生的是 input-&gt;output 的，但 CoT prompt 生成的是
input-&gt;reasoning chain-&gt;output。在 CoT
的过程中，会强迫大模型完成推理，在一系列大模型本来就蕴含的知识之间搭起桥梁，起到激活/串联的效果。
典型的，在指令中加上 "Let's think step by step"
就可以唤醒大模型的推理能力。这就是 "Zero-Shot CoT"。 如果在 prompt
中加上示例，用于指定大模型输入输出的格式，则是 "Few-Shot CoT"。</p>
<h2 id="challenge-and-motivation">Challenge and Motivation</h2>
<p>大模型的部署需要庞大的内存和算力，往往超出了个人或小团队的承载能力。</p>
<p>因此，从业者会选择部署小规模的专用模型。一般由两种方法来得到这些模型：</p>
<ol type="1">
<li>Finetune（微调），通过人工标注的数据来更新预训练的小模型。</li>
<li>Distillation（蒸馏），喂给 teacher model 未标注数据，teacher model
生成标签，来训练 student model。</li>
</ol>
<p>但是，finetune 需要昂贵的人工标注数据，而 Distillation
则需要大量的未标注数据，这一般很难拿到。</p>
<img src="/2024/05/19/Distilling-Step-by-Step/p1.png" class="">
<p>因此，这篇 paper 提出了一种用较少的数据来训练小模型的机制。</p>
<h2 id="methodology">Methodology</h2>
<h3 id="inspiration">Inspiration</h3>
<p>解决上述问题的灵感来自于 CoT(思维链)。</p>
<p>大模型在推理时可以生成一系列的中间步骤（rationales），用于解释最终的输出，我们可以提取这些中间步骤，用于小模型的训练。</p>
<p>给个例子。 问题：有一个长为 15m，宽为 11m 的房间，其中 16
平米已经铺了地砖，请问还要铺多少平米的地砖？ 中间步骤：面积=长 *
宽，这个房间的面积是 15x11 平米。 答案：(11x15)-16</p>
<p>中间步骤会提供更多的额外信息，譬如说面积的计算公式，以往这些信息一般需要更多的数据才能被小模型学会。</p>
<h3 id="extracting-rationalesgenerate-dataset">Extracting
Rationales(Generate Dataset)</h3>
<p>首先需要通过未标注的数据和大模型，得到训练小模型要用的数据（包含
rationale/label）。 这一步很简单，通过在输入前添加 prompt 实现。 prompt
是一个输入输出的样例，包含样例输入，样例输出和
rantionales，告诉大模型需要在生成结果的同时生成 rationales。</p>
<img src="/2024/05/19/Distilling-Step-by-Step/p2.png" class="">
<p>形式化一下：</p>
<ul>
<li>Unlabled dataset: <span
class="math inline">\(D_{unlabled}=\{x_i\}\)</span></li>
<li>Prompt triplet: <span
class="math inline">\(P=(x^P,r^P,y^P)\)</span></li>
<li>LLM output: <span
class="math inline">\((\hat{y_i},\hat{r_i})=LLM(x_i,P)\)</span></li>
<li>Training dataset: <span
class="math inline">\(D=\{(x_i,\hat{y_i},\hat{r_i})\}\)</span></li>
<li>Small model: <span class="math inline">\(f\)</span></li>
</ul>
<h3 id="training-smaller-models-with-rationales">Training smaller models
with rationales</h3>
<p>一般的，如果不使用 rationales，那么蒸馏使用 LLM 生成的 label
来作为训练 label。</p>
<p><span
class="math display">\[\mathcal{L}_{label}=\frac{1}{N}\sum_{i=1}^N
l(f(x_i),\hat{y_i})\]</span></p>
<p>但是，怎么把 rationales 加进 training 过程中呢？</p>
<p>一种直接的思路是将 rationales
作为小模型的额外输入来训练，最近的一些工作就是这么做的。</p>
<p><span class="math display">\[f(x_i,\hat{r_i})\rightarrow
\hat{y_i}\]</span></p>
<p><span class="math display">\[\mathcal{L}=\frac{1}{N}\sum_{i=1}^N
l(f(x_i,\hat{r_i}),\hat{y_i})\]</span></p>
<p>但是，因为小模型需要额外的 rationales
作为输入，所以在推理时大模型仍然是必要的，因为需要先通过大模型生成
rationales，小模型才能产生输出。这与知识蒸馏的初衷背道而驰，因为大模型在部署时仍然是必要的。</p>
<p>因此，这篇 paper 采用了另外一种方法，使小模型同时输出 label 和
rationales。在输入前添加前缀 [lable]/[rationale],当前缀为 [lable] 时输出
<span class="math inline">\(y_i\)</span>，前缀为 [rationale] 时输出
<span class="math inline">\(r_i\)</span>。损失函数则是两者的加权。</p>
<p><span class="math display">\[f(x_i)\rightarrow (y_i,r_i)\]</span>
<span
class="math display">\[\mathcal{L}_{rationale}=\frac{1}{N}\sum_{i=1}^N
l(f(x_i),\hat{r_i})\]</span> <span
class="math display">\[\mathcal{L}=\mathcal{L}_{lable}+\lambda
\mathcal{L}_{rationale}\]</span></p>
<p>与之前的方法相比，小模型部署时不再依赖大模型。并且在训练中学习到了大模型的中间推理步骤。</p>
<img src="/2024/05/19/Distilling-Step-by-Step/p3.png" class="">
<h2 id="results">Results</h2>
<ol type="1">
<li>与传统的 finetune/KD 相比，上述方法可以在仅使用 15%-50%
的训练数据的情况下，达到更好的性能。</li>
<li>与 LLM 相比，可以用更小的模型大小（最小是 LLM 的 1/2000）取得比 LLM
更好的表现。</li>
<li>使用了更少的数据和更小的模型，在性能上超越了 LLM。</li>
</ol>
<img src="/2024/05/19/Distilling-Step-by-Step/p4.png" class="">
<img src="/2024/05/19/Distilling-Step-by-Step/p5.png" class="">
<img src="/2024/05/19/Distilling-Step-by-Step/p6.png" class="">
<img src="/2024/05/19/Distilling-Step-by-Step/p7.png" class="">
<h2 id="参考文献">参考文献</h2>
<p>[1] Distilling Step-by-Step! Outperforming Larger Language Models
with Less Training Data and Smaller Model Sizes, <span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzIzMDUuMDIzMDE=">https://arxiv.org/abs/2305.02301<i class="fa fa-external-link-alt"></i></span>. [2] A Survey on
Knowledge Distillation of Large Language Model, <span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzI0MDIuMTMxMTY=">https://arxiv.org/abs/2402.13116<i class="fa fa-external-link-alt"></i></span>. [3]
一文读懂：大模型思维链 CoT（Chain of Thought）, <span class="exturl" data-url="aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC82NzA5MDc2ODU=">https://zhuanlan.zhihu.com/p/670907685<i class="fa fa-external-link-alt"></i></span>. [4]
知识蒸馏：原理、算法、应用, <span class="exturl" data-url="aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC82MzcxMDg2MTc=">https://zhuanlan.zhihu.com/p/637108617<i class="fa fa-external-link-alt"></i></span>. [5]
论文笔记（LLM+蒸馏）：Distilling step-by-step+代码分析, <span class="exturl" data-url="aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC82NDIzMDAwNzI=">https://zhuanlan.zhihu.com/p/642300072<i class="fa fa-external-link-alt"></i></span>. [6]
大规模语言模型知识蒸馏综述 <span class="exturl" data-url="aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC82OTU2NDAxNjg=">https://zhuanlan.zhihu.com/p/695640168<i class="fa fa-external-link-alt"></i></span>.</p>
]]></content>
      <categories>
        <category>Paper Reading</category>
      </categories>
      <tags>
        <tag>MLSys</tag>
        <tag>LLM</tag>
        <tag>Knowledge Distilation</tag>
      </tags>
  </entry>
  <entry>
    <title>ZeroQuant</title>
    <url>/2024/05/15/ZeroQuant/</url>
    <content><![CDATA[<h2 id="background-knowledge">Background Knowledge</h2>
<h3 id="quantization">Quantization:</h3>
<p>Idea
很简单，就是把模型的参数和运算从浮点数（FP32）映射到较低的精度，比如
INT8。这样做有两点好处。</p>
<ol type="1">
<li>减小了模型的大小，降低了内存开销。</li>
<li>一般来讲 CPU/GPU
做整形运算的速度都比浮点运算快，可以提升推理速度。</li>
</ol>
<p>映射需要在降低模型精度的同时，维持量化之后参数分布与量化之前分布的同构性。其实线性映射就能做的很好了。</p>
<p>如下式所示，将原始值 <span class="math inline">\(r\)</span> 映射到
<span class="math inline">\([q_{min},q_{max}]\)</span></p>
<p><span
class="math display">\[S=\frac{r_{max}-r_{min}}{q_{max}-q_{min}}\]</span>
<span class="math display">\[Z=-\frac{r_{min}}{S}+q_{min}\]</span> <span
class="math display">\[q=\lfloor \frac{r}{S}+Z \rfloor\]</span></p>
<p>其中，<span class="math inline">\(S\)</span> 用于缩放，<span
class="math inline">\(Z\)</span> 用于确定零点。我们需要做的就是确定
<span class="math inline">\(S,Z\)</span>。 可以看出来，在量化的计算中
<span class="math inline">\(r_{range}=r_{max}-r_{min}\)</span>
是至关重要的，量化的结果也是 range 敏感的。
在下面论文的讲解中我们也会用到这点。</p>
<p>模型量化的对象主要有 Weight, Activation, KV cache,
Gradients。(量化的不仅是参数，还有运算)</p>
<h3 id="dequantization">Dequantization:</h3>
<p>模型中一些算子支持低精度表示，那么很好，直接算就行了。</p>
<p>但还有一些算子需要高精度的输入输出（低精度导致很大的误差），就需要将
INT8 反量化为 FP32 再喂给算子计算。</p>
<p><span class="math display">\[r=S(q-Z)\]</span></p>
<p>这会引入误差。</p>
<h3
id="ptqpost-training-quatization">PTQ(Post-training-quatization)</h3>
<p>PTQ，即训练后量化，在模型完成训练之后对模型进行量化。</p>
<p>之前说过，量化需要确定 <span class="math inline">\(S,Z\)</span>，而
<span class="math inline">\(S,Z\)</span> 又由 <span
class="math inline">\(r_{max},r_{min}\)</span> 确定，PTQ
通过选取少量校准数据估算出参数分布，来得到 <span
class="math inline">\(r_{max},r_{min}\)</span>。</p>
<p>以 FP32-&gt;INT8 的 PTQ 为例： 1. 训练出 FP32 的 baseline 模型。 2.
Calibration：使用少量数据估算得到网络各层 weight/activation 的分布，得到
<span class="math inline">\(r_{max},r_{min}\)</span> 3. 算出各层的 <span
class="math inline">\(S,Z\)</span>。 4. 用 <span
class="math inline">\(S,Z\)</span>，将 FP32 的 baseline 量化得到
INT8。</p>
<p>PTQ 运算速度较快，但会损失一定精度。</p>
<h3
id="qatquantization-aware-training">QAT(Quantization-aware-training)</h3>
<p>QAT，即量化感知训练。PTQ 中训练与量化是分开的，但是 QAT
通过在训练时加入伪量化算子，模拟量化带来的误差。</p>
<p>以 FP32-&gt;INT8 的 QAT 为例： 1. 训练出 FP32 的 baseline 模型。 2.
在 baseline model 中加入插入伪量化算子，在数据集上 finetune 得到的 QAT
model。 3. 伪量化算子模拟推理时的量化，并保存 finetune
时得到的量化参数。 4. finetune 完成后，使用量化参数将 QAT model 量化为
INT8 model。</p>
<p>伪量化算子就是量化与反量化算子的结合，模拟 round
带来的误差，定义如下，</p>
<p><span class="math display">\[clamp(r;a,b)=min(max(x,a),b)\]</span>
<span class="math display">\[s(a,b,n)=\frac{b-a}{n-1}\]</span> <span
class="math display">\[q(r;a,b,n)=\lfloor\frac{clamp(r;a,b)-a}{s(a,b,n)}\rfloor
s(a,b,n)+a\]</span></p>
<h2 id="challenge-and-motivation">Challenge and Motivation</h2>
<p>随着大模型的规模急剧增加，大模型的推理面对着两个问题，极高 GPU
内存占用和计算开销。</p>
<p>而解决这一问题的办法之一就是量化。其一可以降低模型的内存占用，其二可以提升模型的运算性能。</p>
<p>然而，为了弥补量化导致的精度损失，通常需要使用 QAT 技术，retrain
模型。这会带来几个问题：</p>
<ol type="1">
<li>训练用的数据往往不是公开的。</li>
<li>重新训练模型所需要很多计算资源。</li>
<li>耗时很久。</li>
</ol>
<p>PTQ 技术就能解决这些问题，因为 PTQ</p>
<ol type="1">
<li>不需要训练数据。</li>
<li>极少的计算资源。</li>
<li>几乎不需要重新训练。</li>
</ol>
<p>最近也有一些关于 PTQ 的量化工作，但是这些工作</p>
<ol type="1">
<li>只关注小规模的 CV 问题。</li>
<li>只局限于高精度量化（INT8/FP16），且只支持 BERT 模型。</li>
<li>不关注量化和反量化开销。（这是性能的重要组成部分）</li>
<li>对于极端量化（如
INT4）或更高的精度，通常使用知识蒸馏，导致额外的开销。</li>
</ol>
<p>我们想要使用 PTQ 技术来解决上述问题，但是直接使用 PTQ
技术是不可行的，会导致 accuracy 的下降。 如图所示，图中 WXAY 代表 Weight
和 Activation 的量化精度。</p>
<img src="/2024/05/15/ZeroQuant/f2.png" class="">
<p>可以看到，INT8 激活量化带来了主要的精度损失，而 weight
量化会导致生成式任务的性能变差。（但 zero-shot 对此不敏感）</p>
<p>为什么 INT8 量化会导致精度损失呢？</p>
<ol type="1">
<li>激活层的 range 是动态变化的，左图显示了对不同 token
而言，激活层输出的 range 有非常大的差异。譬如说 layer11，最小的 range 为
8，最大的 range 为 35。如果对所有 token 使用相同的量化范围的话，range
较小的 token 就会损失很多精度。</li>
<li>Weight 中不同行的 range 差距极大。如右图所示，不同行的 range
之间最多有十倍的差距，但是 PTQ 对整个 Weight Matrix
是一起量化的，取的是整个 Weight 的范围。导致 PTQ 的性能较差。如果使用
INT4 量化（总共 16 个数），那对于 range 较小的行，可能只有 2-3
个数的表示范围。</li>
</ol>
<img src="/2024/05/15/ZeroQuant/f1.png" class="">
<h2 id="methodology">Methodology</h2>
<p>为此，Paper 提出了以下几个技术：</p>
<h3 id="group-wise-quantization-for-weights">Group-wise Quantization for
Weights：</h3>
<ol type="1">
<li>将权重矩阵划分为多个组，每个组分别量化。</li>
<li>针对 GPU 做了优化，降低了推理延迟。</li>
</ol>
<img src="/2024/05/15/ZeroQuant/f4.png" class="">
<h3 id="token-wise-quantization-for-activations">Token-wise Quantization
for Activations</h3>
<ol type="1">
<li>PTQ
一般是静态量化激活的。譬如说离线校准时计算出激活层范围。但是，对于大模型而言，不同
token 的激活范围差异极大，统一静态的量化会导致极大的精度损失。</li>
<li>自然的想法是使用更细粒度的量化策略。这篇 paper 提出了逐 token
量化的技术，动态计算每个 token 的激活值范围，再做量化。</li>
<li>消除了激活范围校准的开销。</li>
<li>但是，逐 token 量化会引入显著的量化和反量化成本，为此，paper
设计了一个高度优化的推理后端。</li>
</ol>
<h3
id="quantization-optimized-transformer-kernels">Quantization-Optimized
Transformer Kernels</h3>
<ol type="1">
<li>在推理过程中，batch-size 较小，因此推理的延迟主要是 device 和 host
之间的内存搬运导致的。</li>
<li>量化本身降低了模型的大小，减少了加载的数据量。但是，量化和反量化运算又导致了额外的开销。</li>
<li>使用 CUTLASS INT8 GeMM（通用矩阵乘）。适配 INT8 计算和 kernel
fuse。</li>
<li>Fusing Token-wise Activation Quantization。使用 kernel fuse 技术，将
Activation 的量化操作与之前的运算融合为一个运算，将反量化与 GeMM
融合为一个运算，由于融合的运算使用的是相同的数据，因此规避了量化/反量化带来的额外内存搬运开销。</li>
</ol>
<img src="/2024/05/15/ZeroQuant/f3.png" class="">
<h3 id="layer-by-layer-knowledge-distillation-lkd">Layer-by-layer
knowledge distillation (LKD)</h3>
<ol type="1">
<li>知识蒸馏可以有效降低量化导致的精度损失。</li>
<li>但是，知识蒸馏会极大增加内存和计算成本，而且往往需要原始训练数据（经常拿不到）。</li>
</ol>
<p>所以，paper 提出了逐层蒸馏技术。假设目标模型有 <span
class="math inline">\(N\)</span> 个 transformer 块 <span
class="math inline">\(L_1,L_2,\dots L_N\)</span>，数据集为 <span
class="math inline">\((X,Y)\)</span>，可以是原始训练数据，也可以来自其他地方。</p>
<p>LKD 使用未量化的模型作为教师模型。若 <span
class="math inline">\(L_k\)</span> 被量化，其量化后的版本为 <span
class="math inline">\(\hat{L_k}\)</span>。那么用 <span
class="math inline">\(L_{k-1}\)</span> 的输出来作为 <span
class="math inline">\(L_k,\hat{L_k}\)</span>的输入，度量学生模型和教师模型的差距，然后更新
<span class="math inline">\(L_k\)</span>。具体就像这个公式一样。</p>
<p><span
class="math display">\[\mathcal{L}_{LKD,k}=MSE(L_k(L_{k-1}(...L_1(X)...))-\hat{L_k}(L_{k-1}(...L_1(X)...)))\]</span></p>
<p>这么做带来了以下好处：</p>
<ol type="1">
<li>LKD 不需要额外的教师模型，而且学生和教师模型共享
L1-Lk-1，其运算结果在之前的迭代中以及知道了，所以额外的运算成本只有
Lk（类似动态规划）。</li>
<li>唯一需要 optimize 的层是 Lk，所以只要将 Lk 加载进内存。</li>
<li>LKD 是逐层优化的，并不优化端到端的模型，所以 LKD
不依赖于原始数据。</li>
</ol>
<h2 id="results">Results</h2>
<ul>
<li>ZeroQuant 可以将 BERT 和 类GPT-3 等模型的权重和激活精度降低到
INT8，对模型准确率的影响最小，同时，与 FP16
推理相比，这些模型的推理速度提高了 5.19 倍/4.16 倍；</li>
<li>ZeroQuant 加上 LKD 可将全连接模块中的权重量化为
INT4，以及注意力模块中的INT8权重和INT8激活，与FP16模型相比，内存占用减少了3倍；</li>
<li>ZeroQuant可以直接应用于GPT-J和GPT-NeoX等，其中我们的INT8模型达到了与FP16模型相似的精度，但效率提高了5.2倍。</li>
</ul>
<h2 id="参考文献">参考文献</h2>
<p>[1] ZeroQuant: Efficient and Affordable Post-Training Quantization
for Large-Scale Transformers, <span class="exturl" data-url="aHR0cHM6Ly9hcnhpdi5vcmcvYWJzLzIyMDYuMDE4NjE=">https://arxiv.org/abs/2206.01861<i class="fa fa-external-link-alt"></i></span>. [2] ZeroQuant
vedio&amp;slides, <span class="exturl" data-url="aHR0cHM6Ly9zbGlkZXNsaXZlLmNvbS8zODk5MTQ4NC96ZXJvcXVhbnQtZWZmaWNpZW50LWFuZC1hZmZvcmRhYmxlLXBvc3R0cmFpbmluZy1xdWFudGl6YXRpb24tZm9yLWxhcmdlc2NhbGUtdHJhbnNmb3JtZXJz">https://slideslive.com/38991484/zeroquant-efficient-and-affordable-posttraining-quantization-for-largescale-transformers<i class="fa fa-external-link-alt"></i></span>.
[3] 大模型压缩首篇综述来啦~, <span class="exturl" data-url="aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC82NTI0MzQxNjU=">https://zhuanlan.zhihu.com/p/652434165<i class="fa fa-external-link-alt"></i></span>. [4]
量化感知训练（Quantization-aware-training）探索-从原理到实践, <span class="exturl" data-url="aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC81NDgxNzQ0MTY=">https://zhuanlan.zhihu.com/p/548174416<i class="fa fa-external-link-alt"></i></span>. [5]
目前针对大模型进行量化的方法有哪些？, <span class="exturl" data-url="aHR0cHM6Ly93d3cuemhpaHUuY29tL3F1ZXN0aW9uLzYyNzQ4NDczMi9hbnN3ZXIvMzI2MTY3MTQ3OA==">https://www.zhihu.com/question/627484732/answer/3261671478<i class="fa fa-external-link-alt"></i></span>.
[6] 大模型量化技术原理-ZeroQuant系列, <span class="exturl" data-url="aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC82ODM4MTM3Njk=">https://zhuanlan.zhihu.com/p/683813769<i class="fa fa-external-link-alt"></i></span></p>
]]></content>
      <categories>
        <category>Paper Reading</category>
      </categories>
      <tags>
        <tag>MLSys</tag>
        <tag>LLM</tag>
        <tag>Quantization</tag>
      </tags>
  </entry>
  <entry>
    <title>MuCache</title>
    <url>/2024/05/15/MuCache/</url>
    <content><![CDATA[<h1 id="microservice-call-graph-caching">Microservice Call Graph
Caching</h1>
<h2 id="task-1-reading">Task #1: Reading</h2>
<h3
id="read-httpswww.cis.upenn.edusga001papersmucache-nsdi24.pdf.-especially-sec-4-sec-5-and-appendix-a-formal-proof-of-correctness.-source-code-httpsgithub.comeniacmucache">1.
Read https://www.cis.upenn.edu/~sga001/papers/mucache-nsdi24.pdf.
Especially Sec 4, Sec 5 and Appendix A formal proof of correctness.
Source code: https://github.com/eniac/mucache</h3>
<p>You can see my reading traces in the annotations of the <a
href="./mucache-nsdi24.pdf">paper</a>.</p>
<h3
id="for-the-example-in-figure-4-draw-the-execution-sequence-similar-to-figure-3-to-show-how-mucache-protocol-solves-the-challenge-of-the-diamond-pattern.-what-if-the-write-and-read-operations-come-from-independent-processes-in-s1-draw-all-possible-execution-sequences.">2.
For the example in Figure 4, draw the execution sequence (similar to
Figure 3) to show how MuCache protocol solves the challenge of the
“diamond” pattern. What if the write and read operations come from
independent processes in S1? Draw all possible execution sequences.</h3>
<h4 id="diamond-graph">Diamond Graph</h4>
<pre class="mermaid">flowchart LR;
    subgraph Service1
    S1-.-> W11[W1]
    S1-.-> W12[W1]
    W12-.->CM1((CM1))
    W11-.->CM1((CM1))
    CM1-.->C1[(C1)]
    W11-.->C1
    end

    subgraph Service2
    S2-.-> W21[W2]
    S2-.-> W22[W2]
    W22-.->CM2((CM2))
    W21-.->CM2((CM2))
    CM2-.->C2[(C2)]
    W21-.->C2
    end

    subgraph Service3
    S3-.-> W31[W3]
    S3-.-> W32[W3]
    W32-.->CM3((CM3))
    W31-.->CM3((CM3))
    CM3-.->C3[(C3)]
    W31-.->C3
    end

    subgraph Service4
    S4-.-> W41[W4]
    S4-.-> W42[W4]
    W42--> D4[(D4)]
    W42-.->CM4((CM4))
    W41-.->CM4((CM4))
    CM4-.->C4[(C4)]
    W41-.->C4
    end
    
    W11-->S2
    CM2-.->CM1
    W11-->S3
    CM3-.->CM1
    W21-->S4
    CM4-.->CM2
    W31-->S4
    CM4-.->CM3</pre>
<h4 id="single-process">Single Process</h4>
<h5 id="call-stack">Call Stack</h5>
<p>Call stack of the example in paper(with MuCache) Firstly S1 read(k)
with trace S1--&gt;S3--&gt;S4(To prepare cache and set status). Then S1
write(k) with trace S1--&gt;S2--&gt;S4. Eventualy S1 read(k) with trace
S1--&gt;S3--&gt;S4.(Same as the first one, so ignore it.)
<figure class="highlight txt"><table><tr><td class="code"><pre><span class="line">W1 preReqStart(ctx[call(&quot;read&quot;,k)])</span><br><span class="line">CM1 startHandler(call(&quot;read&quot;,k)):</span><br><span class="line">    history=[Call(&quot;read&quot;,k)]</span><br><span class="line">W1 preCall(ctx,ca):[ca=Call(&quot;read&quot;,k)]</span><br><span class="line">    readset=&#123;cid:ca&#125;</span><br><span class="line">    cache.get(ca)-&gt;None</span><br><span class="line"></span><br><span class="line">W1 call(&quot;read&quot;,k) to S3</span><br><span class="line">W3 preReqStart(ctx[call(&quot;read&quot;,k)])</span><br><span class="line">CM3 startHandler(call(&quot;read&quot;,k)):</span><br><span class="line">    history=[Call(&quot;read&quot;,k)]</span><br><span class="line">W3 preCall(ctx,ca):[ca=Call(&quot;read&quot;,k)]</span><br><span class="line">    readset=&#123;cid:ca&#125;</span><br><span class="line">    cache.get(ca)-&gt;None</span><br><span class="line"></span><br><span class="line">S3 call(&quot;read&quot;,k) to S4</span><br><span class="line">W4 preReqStart(ctx[call(&quot;read&quot;,k)])</span><br><span class="line">CM4 startHandler(call(&quot;read&quot;,k)):</span><br><span class="line">    history=[Call(&quot;read&quot;,k)]</span><br><span class="line">W4 preRead(ctx,k):</span><br><span class="line">    readsets=&#123;cid:[k]&#125;</span><br><span class="line">W4 read k from DB, get ret</span><br><span class="line">W4 preReturn(ctx, ret)</span><br><span class="line">CM4 endHandler(ca,[k],S2,ret,&#123;S1,S3,S4&#125;):</span><br><span class="line">    saved=&#123;k:(S3,ca)&#125;</span><br><span class="line"></span><br><span class="line">S4 return ret to S3</span><br><span class="line">CM3 saveHandler(ca,ret,&#123;S1,S3,S4&#125;):</span><br><span class="line">    visited=&#123;ca:&#123;S1,S3,S4&#125;&#125;</span><br><span class="line">    cache=&#123;ca:ret&#125;</span><br><span class="line">W3 preReturn(ctx,ret)</span><br><span class="line">CM3 endHandler(ca,[ca],client,ret,&#123;S1,S3,S4&#125;):</span><br><span class="line">    saved=&#123;ca:(S1,ca)&#125;</span><br><span class="line"></span><br><span class="line">S3 return ret to S1</span><br><span class="line">CM1 saveHandler(ca,ret,&#123;S1,S3,S4&#125;):</span><br><span class="line">    visited=&#123;ca:&#123;S1,S3,S4&#125;&#125;</span><br><span class="line">    cache=&#123;ca:ret&#125;</span><br><span class="line">W1 preReturn(ctx,ret)</span><br><span class="line">CM1 endHandler(ca,[ca],None,ret,&#123;S1,S3,S4&#125;):</span><br><span class="line">    saved=&#123;ca:(None,ca)&#125;</span><br><span class="line">S1 return ret</span><br><span class="line"></span><br><span class="line">S1 call(&quot;write&quot;,k,v) to S2(ignore preReqStart &amp; preCall, because &quot;write&quot; is not RO)</span><br><span class="line">S2 call(&quot;write&quot;,k,v) to S4</span><br><span class="line">S4 write(k,v) to DB: </span><br><span class="line">    DB=&#123;k:v&#125;</span><br><span class="line">W4 postWrite(k): </span><br><span class="line">CM4 invHandler(k):</span><br><span class="line">    history=[Call(ca),Inv(k)]</span><br><span class="line">    saved[k]=(S3,ca)</span><br><span class="line">    saved=&#123;&#125;</span><br><span class="line"></span><br><span class="line">CM3 invHandler(ca):</span><br><span class="line">    history=[Call(ca),Inv(ca)]</span><br><span class="line">    saved[ca]=(S1,ca)</span><br><span class="line">    saved=&#123;&#125;</span><br><span class="line">    cache=&#123;&#125;</span><br><span class="line"></span><br><span class="line">CM1 invHandler(ca):</span><br><span class="line">    history=[Call(ca),Inv(ca)]</span><br><span class="line">    saved[ca]=(None,ca)</span><br><span class="line">    saved=&#123;&#125;</span><br><span class="line">    cache=&#123;&#125;</span><br><span class="line"></span><br><span class="line">Then S1 call(&quot;read&quot;,k) would not return the cache unchanged because the cache is empty.</span><br><span class="line"></span><br><span class="line">S1 would execute call(&quot;read&quot;,k) just like the first part of the call stack above.</span><br></pre></td></tr></table></figure></p>
<h5 id="execution-sequence">Execution Sequence</h5>
<figure class="highlight txt"><table><tr><td class="code"><pre><span class="line">(1) C1.get(call(&quot;read&quot;,k))-&gt;None</span><br><span class="line">(2a) S1 call(&quot;read&quot;,k) to S3</span><br><span class="line">(2b) CM1.Start(call(&quot;read&quot;,k))</span><br><span class="line">(3) C3.get(call(&quot;read&quot;,k))-&gt;None</span><br><span class="line">(4a) S3 call(&quot;read&quot;,k) to S4</span><br><span class="line">(4b) CM3.Start(call(&quot;read&quot;,k))</span><br><span class="line">(5) CM4.Start(call(&quot;read&quot;,k))</span><br><span class="line">(6) Read(k)</span><br><span class="line">(7a) S4 return ret to S3</span><br><span class="line">(7b) CM4 End(call(&quot;read&quot;,k),&#123;k&#125;,ret)</span><br><span class="line">(8) CM3.Save call(&quot;read&quot;,k)-&gt;ret</span><br><span class="line">(9) C3.set(call(&quot;read&quot;,k))-&gt;ret</span><br><span class="line">(9a) S3 return ret to S1</span><br><span class="line">(9b) CM3 End(call(&quot;read&quot;,k),&#123;call(&quot;read&quot;,k)&#125;,ret)</span><br><span class="line">(10) CM1.Save call(&quot;read&quot;,k)-&gt;ret</span><br><span class="line">(11) C1.set(call(&quot;read&quot;,k))-&gt;ret</span><br><span class="line">(12) return ret</span><br><span class="line"></span><br><span class="line">(13) S1 call(&quot;write&quot;,k,v) to S2</span><br><span class="line">(14) S2 call(&quot;write&quot;,k,v) to S4</span><br><span class="line">(15) S4 Write(k,v)</span><br><span class="line">(16) CM4.Inv(k)</span><br><span class="line">(17) CM3.Inv(call(&quot;read&quot;,k)) </span><br><span class="line">(18) C3.delete(call(&quot;read&quot;,k))</span><br><span class="line">(19) CM1.Inv(call(&quot;read&quot;,k))</span><br><span class="line">(20) C1.delete(call(&quot;read&quot;,k))</span><br><span class="line"></span><br><span class="line">(21) C1.get(call(&quot;read&quot;,k))-&gt;None</span><br><span class="line">(22) ......(Same as (1)-(12))</span><br></pre></td></tr></table></figure>
<h4 id="multiple-independent-process">Multiple Independent Process</h4>
<pre class="mermaid">flowchart LR
    S1-->|W|S2
    S2-->|W|S4
    S1-->|R|S3
    S3-->|R|S4</pre>
<p>Ignore the equivalent modulo reordering traces.</p>
<p>If, as in the case of a single process, there is a cache for
call("read", k) in S1, then there are only two possibilities:</p>
<ol type="1">
<li>After Inv propagation back to S1, the Read propagation follows the
trace S1 -&gt; S3 -&gt; S4, consistent with the trace of the single
process mentioned earlier.</li>
<li>If Inv has not yet propagated back to S1, then the read operation
directly returns the content from the cache. <figure class="highlight txt"><table><tr><td class="code"><pre><span class="line">Write</span><br><span class="line">......</span><br><span class="line">Read(k)</span><br><span class="line">C1.get(ca)--&gt;ret</span><br><span class="line">S1 return ret</span><br><span class="line">......</span><br><span class="line">C1.Inv(ca)</span><br><span class="line">......</span><br></pre></td></tr></table></figure></li>
</ol>
<p>Let's consider the case where there is no cache entry for read(k) in
S1.</p>
<ol type="1">
<li>In CM4, if Inv(k) arrives before End(read...) is reached, then the
information about the cache entry for k is not yet recorded in saved,
and Inv(k) cannot propagate along S4--&gt;S3--&gt;S1. Also, it cannot
send Save information to S3. However, since CM3 has not received
Inv(ca), it sends Save to CM1. <figure class="highlight txt"><table><tr><td class="code"><pre><span class="line">(1) C1.get(call(&quot;read&quot;,k))-&gt;None</span><br><span class="line">(2a) S1 call(&quot;read&quot;,k) to S3</span><br><span class="line">(2b) CM1.Start(call(&quot;read&quot;,k))</span><br><span class="line">(3) C3.get(call(&quot;read&quot;,k))-&gt;None</span><br><span class="line">(4a) S3 call(&quot;read&quot;,k) to S4</span><br><span class="line">(4b) CM3.Start(call(&quot;read&quot;,k))</span><br><span class="line">(5) CM4.Start(call(&quot;read&quot;,k))</span><br><span class="line">(6) Read(k)-&gt;ret</span><br><span class="line"></span><br><span class="line">(7) S1 call(&quot;write&quot;,k,v) to S2</span><br><span class="line">(8) S2 call(&quot;write&quot;,k,v) to S4</span><br><span class="line">(9) S4 Write(k,v)</span><br><span class="line"></span><br><span class="line">(1)-(6) and (7)-(9) 是同步独立进行的。</span><br><span class="line"></span><br><span class="line">(10) Inv(k)</span><br><span class="line">(11a) CM4 End(call(&quot;read&quot;,k),&#123;k&#125;,ret)</span><br><span class="line">(11b) S4 return ret to S3</span><br><span class="line"></span><br><span class="line">(12a) S3 return ret to S1</span><br><span class="line">(12b) CM3 End(call(&quot;read&quot;,k),&#123;call(&quot;read&quot;,k)&#125;,ret)</span><br><span class="line">(13) CM1.Save call(&quot;read&quot;,k)-&gt;ret</span><br><span class="line">(14) C1.set(call(&quot;read&quot;,k))-&gt;ret</span><br><span class="line">(15) return ret</span><br></pre></td></tr></table></figure></li>
<li>In CM4, if Inv(k) arrives after End(read...), then let's consider
the situation in CM3 (keeping in mind that Save and Inv cannot be
reordered, so Save should arrive before Inv in CM3, and similarly in
CM1): <figure class="highlight txt"><table><tr><td class="code"><pre><span class="line">(1) C1.get(call(&quot;read&quot;,k))-&gt;None</span><br><span class="line">(2a) S1 call(&quot;read&quot;,k) to S3</span><br><span class="line">(2b) CM1.Start(call(&quot;read&quot;,k))</span><br><span class="line">(3) C3.get(call(&quot;read&quot;,k))-&gt;None</span><br><span class="line">(4a) S3 call(&quot;read&quot;,k) to S4</span><br><span class="line">(4b) CM3.Start(call(&quot;read&quot;,k))</span><br><span class="line">(5) CM4.Start(call(&quot;read&quot;,k))</span><br><span class="line"></span><br><span class="line">(6) S1 call(&quot;write&quot;,k,v) to S2</span><br><span class="line">(7) S2 call(&quot;write&quot;,k,v) to S4</span><br><span class="line"></span><br><span class="line">(8) Read(k)</span><br><span class="line">(9) S4 Write(k,v)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">(10) CM4 End(call(&quot;read&quot;,k),&#123;k&#125;,ret)</span><br><span class="line">(11) S4 return ret to S3</span><br><span class="line">(12) Inv(k)</span><br></pre></td></tr></table></figure>
<ul>
<li><p>Save--&gt;Inv--&gt;End <figure class="highlight txt"><table><tr><td class="code"><pre><span class="line">(13) CM3.Save(ca)-&gt;ret;C3.set(ca,ret)</span><br><span class="line">(14) CM3.Inv(ca);C3.delete(ca)</span><br><span class="line">(15) CM3.End(ca,...)</span><br><span class="line">(16) S3 return ret to S1</span><br><span class="line">(17) CM1.End(ca,...)</span><br><span class="line">(18) return ret</span><br></pre></td></tr></table></figure></p></li>
<li><p>End--&gt;Save--Inv <figure class="highlight txt"><table><tr><td class="code"><pre><span class="line">(13) CM3.End(ca,...)</span><br><span class="line">(14) CM3.Save(ca)-&gt;ret;C3.set(ca,ret)</span><br><span class="line">(15) CM3.Inv(ca);C3.delete(ca)</span><br><span class="line">(16) S3 return ret to S1</span><br><span class="line">(17) CM1.End(ca,...)</span><br><span class="line">(18) CM1.Save(ca)-&gt;ret;C1.set(ca,ret)</span><br><span class="line">(19) CM1.Inv(ca);C1.delete(ca)</span><br><span class="line">(20) return ret</span><br></pre></td></tr></table></figure></p></li>
</ul></li>
</ol>
<h3
id="choose-a-microservice-and-provide-a-few-example-function-calls-to-simulate-which-data-could-be-cached-and-simulate-a-cache-hit-and-a-cache-miss.-you-may-choose-any-cache-size-and-eviction-policy-you-like">3.
Choose a microservice and provide a few example function calls to
simulate which data could be cached, and simulate a cache hit and a
cache miss. (You may choose any cache size and eviction policy you
like)</h3>
<h4 id="initialization">Initialization</h4>
<p>Cache size: 2 Eviction policy: LRU Microservice:</p>
<pre class="mermaid">classDiagram
class client
class frontend
class pic_manager
class text_manager

class text_database
class pic_database
    
client-->frontend
frontend-->pic_manager
pic_manager-->pic_database
frontend-->text_manager
text_manager-->text_database</pre>
<p>All caches and states are empty at the beginning.</p>
<h4 id="function-sequence">Function Sequence</h4>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">   download_text(name1)</span><br><span class="line">--&gt;download_text(name2)</span><br><span class="line">--&gt;download_pic(name3)</span><br><span class="line">--&gt;uploadpic(name3,pic2)</span><br><span class="line">--&gt;download_text(name2)</span><br></pre></td></tr></table></figure>
<h4 id="memory-states">Memory States</h4>
<p>Only simulate the memory states.</p>
<pre class="mermaid">---
title: Init
---
classDiagram
class client
class frontend
class pic_manager
class text_manager

class text_database
text_database: name1->txt1
text_database: name2->txt2
class pic_database
pic_database: name3->pic1
    
client-->frontend
frontend-->pic_manager
pic_manager-->pic_database
frontend-->text_manager
text_manager-->text_database</pre>
<pre class="mermaid">---
title: download_text(name1), cache miss
---
classDiagram

class client
client: download_text(name1)->txt1
class frontend
frontend: download_text(name1)->txt1
class pic_manager
class text_manager
text_manager: download_text(name1)->txt1

class text_database
text_database: name1->txt1
text_database: name2->txt2
class pic_database
pic_database: name3->pic1


client-->frontend:download
frontend-->pic_manager
pic_manager-->pic_database
frontend-->text_manager:download
text_manager-->text_database:download
text_database-->text_manager:save
text_manager-->frontend:save
frontend-->client:save</pre>
<pre class="mermaid">---
title: download_text(name2), cache miss
---
classDiagram

class client
client: download_text(name1)->txt1
client: download_text(name2)->txt2
class frontend
frontend: download_text(name1)->txt1
frontend: download_text(name2)->txt2

class pic_manager
class text_manager
text_manager: download_text(name1)->txt1
text_manager: download_text(name2)->txt2

class text_database
text_database: name1->txt1
text_database: name2->txt2
class pic_database
pic_database: name3->pic1


client-->frontend:download
frontend-->pic_manager
pic_manager-->pic_database
frontend-->text_manager:download
text_manager-->text_database:download
text_database-->text_manager:save
text_manager-->frontend:save
frontend-->client:save</pre>
<pre class="mermaid">---
title: download_pic(name3), cache miss/eviction
---
classDiagram

class client
client: download_text(name2)->txt2
client: download_pic(name3)->pic1
class frontend
frontend: download_text(name2)->txt2
frontend: download_pic(name3)->pic1

class pic_manager
pic_manager: download_pic(name3)->pic1
class text_manager
text_manager: download_text(name1)->txt1
text_manager: download_text(name2)->txt2

class text_database
text_database: name1->txt1
text_database: name2->txt2
class pic_database
pic_database: name3->pic1


client-->frontend:download
frontend-->pic_manager:download
pic_manager-->pic_database:download
frontend-->text_manager
text_manager-->text_database
pic_database-->pic_manager:save
pic_manager-->frontend:save
frontend-->client:save</pre>
<pre class="mermaid">---
title: uploadpic(name3,pic2), cache invalidate
---
classDiagram

class client
client: download_text(name2)->txt2
class frontend
frontend: download_text(name2)->txt2

class pic_manager
class text_manager
text_manager: download_text(name1)->txt1
text_manager: download_text(name2)->txt2

class text_database
text_database: name1->txt1
text_database: name2->txt2
class pic_database
pic_database: name3->pic2


client-->frontend:upload
frontend-->pic_manager:upload
pic_manager-->pic_database:upload
frontend-->text_manager
text_manager-->text_database
pic_database-->pic_manager:inv
pic_manager-->frontend:inv
frontend-->client:inv</pre>
<pre class="mermaid">---
title: download_text(name2), cache hit
---
classDiagram

class client
client: download_text(name2)->txt2
class frontend
frontend: download_text(name2)->txt2

class pic_manager
class text_manager
text_manager: download_text(name1)->txt1
text_manager: download_text(name2)->txt2

class text_database
text_database: name1->txt1
text_database: name2->txt2
class pic_database
pic_database: name3->pic2


client-->frontend
frontend-->pic_manager
pic_manager-->pic_database
frontend-->text_manager
text_manager-->text_database</pre>
<h2 id="task-2-coding">Task #2: Coding</h2>
<blockquote>
<p>Summarize the key components to apply the MuCache protocol, and
implement an example to show that the datastore is linearizable. Note:
We are not asking you to build and run the open-source version of
MuCache (you may use it as a reference). Please implement your own
version of MuCache protocol (just a prototype to show the key ideas).
You may remove any component you think is not critical in the design,
and use simple functions to simulate the behavior of each service/thread
in your code.</p>
</blockquote>
<p>Please refer to the code <a href="./mucache">mucache</a> and <a
href="./mucache/README.md">readme</a>.</p>
]]></content>
      <categories>
        <category>Paper Reading</category>
      </categories>
      <tags>
        <tag>MicroService</tag>
      </tags>
  </entry>
  <entry>
    <title>Parsimon</title>
    <url>/2024/05/15/Parsimon/</url>
    <content><![CDATA[<h1 id="fast-flow-level-network-simulation">Fast Flow-level Network
Simulation</h1>
<h2 id="task-1-reading-writing">Task #1 Reading &amp; Writing:</h2>
<h3 id="read-httpsarxiv.orgpdf2205.01234.pdf">1.1. Read
https://arxiv.org/pdf/2205.01234.pdf</h3>
<p>You can see my reading traces in the annotations of the <a
href="./parsimon-nsdi23.pdf">paper</a>.</p>
<h3
id="conclude-the-key-idea-of-the-paper-explain-the-algorithm-and-analyze-its-sources-of-speedup-and-sources-of-error.">1.2.
Conclude the key idea of the paper, explain the algorithm, and analyze
its sources of speedup and sources of error.</h3>
<h4 id="key-idea-the-algorithm">1.2.1 Key Idea &amp; The Algorithm</h4>
<p>假设： 1. Data center 中拥塞通常不是同时出现的，而是零散、突发的。 2.
交换机队列的状态近似符合排队论的结果，仅与网络流量有关，所以我们可以独立分析每个队列。</p>
<p>因此，我们可以对路径上每个 link
处的拥塞时间建模。当近似模拟在大规模网络上运行特定工作负载时端到端流量性能的分布时，我们可以聚合路径拥塞分布，以得到总的性能分布。</p>
<p>具体而言，算法步骤如下（做了一定的简化，因为会在后面的数学形式化部分详细谈到）：</p>
<ol type="1">
<li>Decomposition:
<ul>
<li>生成每个 link 的 workload: 包含所有经过该 link 的 workflow</li>
<li>生成 link-level topology</li>
</ul></li>
<li>Clustering: 根据每个 link 的 workload，对 workload 相似的 link
聚类。</li>
<li>Simulation: 从每个 cluster 中抽样一个 link 作为代表，对该 link 及其
workload 做 link-level simulation，得到该 link delay 关于 flowsize
的桶分布，该 cluster 中其他 link 的分布与代表 link 相同。</li>
<li>Aggregation: 给定一条具有 size，path 的 workflow，对 path 中的每个
link，query link 的桶分布（用 size）得到对应的
bucket，再用蒙特卡洛采样，从该 bucket 取出一个 delay 作为该 link
的delay。将路径中所有 link 的 delay 累加，就得到该 workload 的
delay。重复多次，就可以得到 flow-level distribution.</li>
</ol>
<h4 id="sources-of-speedup">1.2.2. Sources of Speedup</h4>
<ol type="1">
<li>将 flow-level simulation 拆解成了独立的 link-level
simulation，可以并行化处理，允许扩展模拟网络的大小和处理核心的数量。</li>
<li>在建立 flow-level topology 时，运用了独特的技术，使得 topology
最多有 3-hop，降低了 simulation 的规模。</li>
<li>没有对所有 link 做 link-level simulation，而是做了
clustering，从中选取一个代表做 link-level simulation。</li>
<li>Aggregation 时，没有对路径上的所有分布做 convolution，而是 sample
出一个点做累加。</li>
</ol>
<h4 id="sources-of-error">1.2.3. Sources of Error</h4>
<ol type="1">
<li>Bottleneck fan-in: Parsimon 在构建 link-level topology 时，将
potential source hosts 直接连到了目标 link，并没有模拟 upstream
的实际情况。这导致模拟中的拥塞和排队情况比实际中更重，因为并没有真实的
upstream links 来 smoothing burst flow。另外，这会略微高估 downstream
造成的延迟。</li>
<li>Lack of traffic smoothing: 在真实网络中，前往目标 link 的
cross-traffic 会被 smoothed，但因为 link-level topology 里无
cross-traffic，会在 simulation 时稍微高估目标 link 的排队延迟。</li>
<li>Link-level independence: Parsimon 假设 link-level simulation
可以独立处理，但这忽略了 flow path 上各 hop
的流量相关性，引入了误差。</li>
<li>One bottleneck at a time: 当拥塞是间歇和暂时的，不同时候出现在不同的
link
上时，Parsimon更准确；当拥塞跨越给定路径的多个边缘和核心链路时，准确性较低。导致高估
long flow end-to-end latency。</li>
<li>Clustering: 由于在 simulation 中使用了 clustering，引入了 actual
link delay distribution 与 representative link delay distribution
之间的误差。</li>
<li>Sampling: Aggregation 时，是从 link-level daley distribution 中
sample 一个点，而非对整个 delay 做 convolution。</li>
</ol>
<h3
id="how-would-you-utilize-machine-learning-algorithms-to-make-flow-level-network-simulation-faster-and-more-precise">1.3.
How would you utilize machine learning algorithms to make flow-level
network simulation faster and more precise?</h3>
<ol type="1">
<li>在处理 link-level distribution 时，运用 machine learning
algorithm(maybe mlp)，为 每个 link train 出一个以 workflow information
为输入，delay 为输出的函数。当针对给定 workflow 做 aggregation
时，使用这个函数来得到该 workflow 在对应 link 的 delay。</li>
<li>改进 clustering 算法，使用诸如 K-means 的算法。</li>
</ol>
<h2 id="task-2-math-formulation">Task #2 Math formulation:</h2>
<blockquote>
<p>In section 3, the paper shows how Parsimon decomposes the network
simulation into a series of link-level simulations, and how it
aggregates the result, please formulate this process. (You can summarize
the fast link-level simulation algorithm in section 4.1 as a function
without further explanation. You are also allowed to ignore some of the
technique details in your formulation)</p>
</blockquote>
<p><span class="math inline">\(\text{Given network topology as
undirected graph }G=(V,E).\)</span></p>
<p><span class="math inline">\(\text{Given workflow routes
}WF.\)</span></p>
<h3 id="generating-link-level-workloads">2.1. Generating Link-Level
Workloads</h3>
<p><span class="math inline">\(\forall link \in
E,(u,v)=link.nodes,WL_{&lt;u,v&gt;}=\{wf|wf \in WF \wedge (\exists
i&lt;j\text{, s.t. }wf.nodes[i]=u\wedge wf.nodes[j]=v)\}\)</span> <span
class="math inline">\(WL_{&lt;v,u&gt;}=\{wf|wf \in WF \wedge (\exists
i&gt;j\text{, s.t. }wf.nodes[i]=u\wedge wf.nodes[j]=v)\}\)</span></p>
<h3 id="generating-link-level-topologies">2.2. Generating Link-Level
Topologies</h3>
<p><span class="math inline">\(\forall link\in E,
(u,v)=link.nodes\)</span></p>
<p><span class="math inline">\(\text{The link-level toplogy
}LinkTopo_{&lt;u,v&gt;}=(V_{&lt;u,v&gt;},E_{&lt;u,v&gt;}).\)</span></p>
<p>Suppose the traffic through the link <span
class="math inline">\(&lt;u,v&gt;\)</span> originates from sources <span
class="math inline">\(S_{&lt;u,v&gt;}\)</span> and terminates in
destinations <span class="math inline">\(T_{&lt;u,v&gt;}\)</span> .</p>
<p><span class="math inline">\(\text{Let }S_{&lt;u,v&gt;}:=\{wf.begin|
wf \in WL_{&lt;u,v&gt;}\},wf.begin=wf[0].\)</span></p>
<p><span class="math inline">\(\text{Let }T_{&lt;u,v&gt;}:=\{wf.end|wf
\in WL_{&lt;u,v&gt;}\},wf.end=wf[wf.length-1].\)</span></p>
<p><span class="math inline">\(\text{If }u.type=host\wedge
v.type=switch,V_{&lt;u,v&gt;}=\{u,v\}\cup
T_{&lt;u,v&gt;},E_{&lt;u,v&gt;}=\{newLink(u,v,mainlink)\} \cup
\{newLink(v,h,downstream)|h \in T_{&lt;u,v&gt;}\}\text{, function
newLink generates a new link object.}\)</span></p>
<p><span class="math inline">\(\text{If }u.type=switch\wedge
v.type=switch,V_{&lt;u,v&gt;}=\{u,v\}\cup T_{&lt;u,v&gt;}\cup
S_{&lt;u,v&gt;},E_{&lt;u,v&gt;}=\{newLink(u,v,mainlink)\} \cup
\{newLink(v,h,downstream)|h \in T_{&lt;u,v&gt;}\}\cup \{
newLink(h,u,upstream)|h\in S_{&lt;u,v&gt;} \}.\)</span></p>
<p><span class="math inline">\(\text{If }u.type=switch\wedge
v.type=host,V_{&lt;u,v&gt;}=\{u,v\}\cup
S_{&lt;u,v&gt;},E_{&lt;u,v&gt;}=\{newLink(u,v,ismainlink)\} \cup
\{newLink(h,u,upstream)|h \in S_{&lt;u,v&gt;}\}.\)</span></p>
<p><span class="math inline">\(LinkTopo_{&lt;v,u&gt;} \text{ would be
genareted by the same way above.}\)</span></p>
<h4 id="modeling-round-trip-delay">2.2.1. Modeling Round-Trip Delay</h4>
<p><span class="math inline">\(\forall link \in
E_{&lt;u,v&gt;}.\)</span></p>
<p><span class="math inline">\(\text{Then the link has the only
correspond workflow }wf\text{, s.t. }wf\in WL_{&lt;u,v&gt;}\wedge
(\exists i&lt;j, wf.nodes[i]=u \wedge wf.nodes[j]=v).\)</span></p>
<p><span class="math inline">\(\text{Therefore we could got the latency
of the link
}link.latency=\sum_{k=i}^{j-1}getLink(wf.nodes[k],wf.nodes[k+1]).latency.\)</span></p>
<p><span class="math inline">\(\text{Let }getLink(u,v):=e \text{ if
}e\in E \wedge e.nodes=(u,v).\)</span></p>
<h4 id="selecting-link-bandwidths">2.2.2. Selecting Link Bandwidths</h4>
<p><span class="math inline">\(\text{We should increase the bandwidth of
downstream links.}\)</span></p>
<p><span class="math inline">\(\forall link \in
E_{&lt;u,v&gt;}.\)</span></p>
<p><span class="math inline">\(\text{If }link.type=downstream\text{,
then }link.bandwidth=addBandwidth(link)\text{, the function addBandwidth
would increase the bandwidth of the link.}\)</span></p>
<h4 id="correcting-for-ack-traffic">2.2.3. Correcting for ACK
Traffic</h4>
<p>This part is too detailed, so I omitted it.</p>
<h3 id="post-processing-link-level-results">2.3. Post-Processing
Link-Level Results</h3>
<p><span class="math inline">\(\forall link \in
E,(u,v)=link.nodes.\)</span></p>
<p><span class="math inline">\(\forall wf \in
WL_{&lt;u,v&gt;}.\)</span></p>
<p><span class="math inline">\(\text{We generate the FCT of the workflow
in the link, }
FCT_{&lt;u,v&gt;,wf}:=Simulate(LinkTopo_{&lt;u,v&gt;},wf).\)</span></p>
<p><span class="math inline">\(\text{Let ideal FCT of the link for the
workflow
}IdealFCT_{&lt;u,v&gt;,wf}:=link.latency+wf.size/link.bandwidth.\)</span></p>
<p><span class="math inline">\(\text{Let latency of the link for the
workflow
}Delay_{&lt;u,v&gt;,wf}:=FCT_{&lt;u,v&gt;,wf}-IdealFCT_{&lt;u,v&gt;,wf}\)</span>.</p>
<h4 id="packet-normalized-delay">2.3.1. Packet-Normalized Delay</h4>
<p><span class="math inline">\(\text{Let packet-normalized delay
}NormDelay_{&lt;u,v&gt;,wf}:=Delay_{&lt;u,v&gt;,wf}/wf.packetsize.\)</span></p>
<h4 id="bucketing-distributions">2.3.2. Bucketing Distributions</h4>
<p><span class="math inline">\(\text{Let set of packet-normalized delay
for the link of each workflow
}NormDelay_{&lt;u,v&gt;}:=\{NormDelay_{&lt;u,v&gt;,wf}|wf \in
WL_{&lt;u,v&gt;}\}.\)</span></p>
<p><span class="math inline">\(\text{Given hyper-parameters
x,B}\)</span>.</p>
<p>For each bucket <span class="math inline">\(b\)</span>, let <span
class="math inline">\(maxf_b\)</span> and <span
class="math inline">\(minf_b\)</span> be the maximum and minimum flow
sizes associated with <span class="math inline">\(b\)</span>,
respectively, and let <span class="math inline">\(n_b\)</span> be the
number of elements in <span class="math inline">\(b\)</span>. Each
bucket <span class="math inline">\(b\)</span> apart from the last one is
locally subject to two constraints:</p>
<p><span class="math display">\[n_b \ge B \text{ and } maxf_b \ge
x*minf_b\]</span></p>
<p>The algorithm to generate buckets and bucketing distribution:</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">global</span> B,x</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">GenerateBuckets</span>(<span class="params">NormDelay</span>):</span><br><span class="line">    BD=[]</span><br><span class="line">    NormDelay.sort(by=wf_size) <span class="comment"># sorted by the flow size, not packet size</span></span><br><span class="line">    nb=<span class="number">0</span></span><br><span class="line">    maxfb=INT_MIN</span><br><span class="line">    minfb=INT_MAX</span><br><span class="line">    bucket=[]</span><br><span class="line">    <span class="keyword">for</span> delay <span class="keyword">in</span> NormDelay:</span><br><span class="line">        bucket.append(delay)</span><br><span class="line">        maxfb=<span class="built_in">max</span>(delay,maxfb)</span><br><span class="line">        minfb=<span class="built_in">min</span>(delay,minfb)</span><br><span class="line">        nb=nb+<span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> nb&gt;=B <span class="keyword">and</span> maxfb&gt;=x*minfb:</span><br><span class="line">            BD.append(copy.deepcopy(bucket))</span><br><span class="line">            bucket=[]</span><br><span class="line">            nb=<span class="number">0</span></span><br><span class="line">            maxfb=INT_MIN</span><br><span class="line">            minfb=INT_MAX</span><br><span class="line">    <span class="keyword">return</span> BD</span><br></pre></td></tr></table></figure>
<p><span class="math inline">\(\text{Then we get the bucketing
distribution of packet-normalized delay for the link
}BD_{&lt;u,v&gt;}:=\{Bucket_{&lt;u,v&gt;,i}\}=GenerateBuckets(NormDelay_{&lt;u,v&gt;}).\)</span></p>
<h3 id="aggregating-link-level-estimates">2.4. Aggregating Link-Level
Estimates</h3>
<p><span class="math inline">\(\text{Given }size,S,D \text{ as size,
source and destination, then Parsimon compute the path from source to
destination }Path=computePath(size,S,D,G).\)</span></p>
<p><span class="math inline">\(\forall i, \vec{v_i}=&lt;u_i,v_i&gt;
=Path[i].\)</span></p>
<p><span class="math inline">\(\text{By querying the bucketing
distribution by } size\text{, we obtain }\)</span></p>
<p><span class="math inline">\(Bucket_i\in BD_{&lt;u_i,v_i&gt;}\text{,
s.t. }size\ge min(BD_{&lt;u_i,v_i&gt;}) \wedge size\le
max(BD_{&lt;u_i,v_i&gt;}).\)</span></p>
<p><span class="math inline">\(\text{Then randomly sample a point from
the bucket }D_i^* \stackrel{}{\leftarrow} Bucket_i.\)</span></p>
<p><span class="math inline">\(\text{The end-to-end absolute delay } D
\text{ is computed as }D=P\sum_{i}D_i^*\text{, where } P \text{ is the
input flow size in packets.}\)</span></p>
<h2 id="task-3-programming">Task #3 Programming:</h2>
<blockquote>
<p>Can you implement and test Algorithm 1 with a sample network? You may
also refer to Appendix D for some details.</p>
</blockquote>
<p>Please refer to the test code <a
href="./algorithm.ipynb">algorithm.ipynb</a> and the source code <a
href="./clustering/">clustering</a>.</p>
]]></content>
      <categories>
        <category>Paper Reading</category>
      </categories>
      <tags>
        <tag>Network</tag>
        <tag>Network Simulation</tag>
      </tags>
  </entry>
</search>
